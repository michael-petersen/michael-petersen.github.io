<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Peer Review</title>

  <link rel="stylesheet" href="../css/main.css">

  <!-- The icon marking the tab -->
  <link rel="shortcut icon" href="../papers/shadowbar/iconimg.png" type="image/png">

</head>

<body class="home color-1">
  <div id="headerstatic">
    <div class="container">
      <div class="row">
      	<h2 id="logostatic">
          <a href="../index.html">
      	    <span>Michael S. Petersen</span>
      	    <span class="highlightstatic">University of Edinburgh</span>
      	  </a>
        </h2>
      </div>
    </div>
  </div>
  <div class="section">
    <div class="container">
      <div class="content">
        <div class="row">
	        <div class="title">
            <h4><a href="list.html">(back to notes list)</a></h4>
          </div>
          <div class="title">
            <h2>Peer Review</h2>
        	    <p>Peer review is an incredibly important part of the scientific process. When functioning well, the community gains high levels of confidence about science products. When functioning badly, peer review has the capacity to grind the science machine completely to a halt. I've personally had mixed experiences on both sides of the process, and I'm starting to try and articulate what makes a good or bad process. I think we're incredibly privileged as scientists to produce work that will exist in perpetuity (particularly in our hyper-connected modern age). Therefore, we should endeavor to create the best products and the best experience. This post is <i>not</i> about the quality of science, per se (although that's inescapable), but rather about the reviewing experience.</p>
              <br>
              <p>I'm going to try and do a couple of things. First, consolidate the timeline data on how the process has gone for different papers. A lot of the delay has been on me, as is readily apparent. If you're reading this and think your process is taking a long time, don't worry, I've taken longer.</p>
              <br>
              <p>I'm also hoping to use this post as a jumping off place for various library science considerations. While finishing up my PhD, I had an opportunity to give a talk relating to library science, and I found a whole world of astronomy that I didn't know about before. For a taste, check out the output of <a href="https://ui.adsabs.harvard.edu/search/q=%20%20author%3A%22%5Ekurtz%2C%20m%22&sort=date%20desc%2C%20bibcode%20desc&p_=0">Michael Kurtz</a>. Other systems, like Altmetric, are gaining popularity, and could possibly be used as ranking metrics in the future.</p>
              <br>
              <p>My long-term (humble) goal is to shift the evaluation of scientists and science somewhat -- as well as how science is disseminated. Peer review is a direct evaluation of scientists on some level; we also all already know that citations and paper counts (and h-index) have become the primary currency of grading someone's science. One additional avenue has (of course) begun to be explored: <a href="https://ui.adsabs.harvard.edu/abs/2020NatAs...4..711K/abstract">machine learning for peer review</a>.</p>
              <br>
              <p>This post was inspired by a <a href="https://twitter.com/rareflwr41/status/1505917366961647623">Twitter thread</a> that I read recently discussing the peer-review process and the role antagonistic reviewers could play in gatekeeping. I'm not totally sure I agree, but the discussion points are thought-provoking.</p>
              <br>
              <p>It is also generally acknowledged (in my opinion) that getting in to refereeing is hard, and very little constructive advice is given. One recent example giving some rudimentary information is <a href="https://arxiv.org/abs/2205.14270">Ntampaka et al. (2022)</a>. There is also a looming crisis in general hampering 'good' reviewing practices, discussed <a href="https://www.insidehighered.com/news/2022/06/13/peer-review-crisis-creates-problems-journals-and-scholars">here</a>.</p>
              <br>
              <p>Does house style in journals matter? What is the point of journal impact factors, and should we pay attention to them?</p>
        	</div>
        </div>
      </div>
    </div>
  </div>

  <div id="footer"> michael.petersen@roe.ac.uk </div>

</body>
</html>

<!--
MNRAS
(1) Shadow bar. Submitted 10 Feb 2016, report 23 Feb 2016 (552 words). Resubmitted 12 Jul 2016, report 4 Aug 2016 (100 words). Resubmitted (2) 15 Aug 2016, accepted 23 Aug 2016. Published 1 Sep 2016 (December 2016 edition)
(2) LMT. Submitted 2 Nov 2018, report 28 Nov 2018. Resubmitted 23 May 2019, report 11 Jun 2019. Resubmitted (2) 13 Jun 2019, accepted 17 Jun 2019. Published 3 July 2019 (September 2019 edition)
(3) Commensurabilities. Submitted 13 Feb 2019, report 21 Mar 2019 (4955 words). Resubmitted 25 Sep 2019, report 14 Nov 2019 (2426 words). Resubmitted (2) 15 Feb 2020, report 5 Mar 2020 (768 words). Resubmitted (3) 15 Jun 2020, report 22 Jun 2020 (433 words). Resubmitted (4) 11 Aug 2020, accepted 28 Sep 2020. Split into two papers (see 6). Published 16 Oct 2020 (January 2021 edition).
(4) Torque. Submitted 6 Mar 2019, report 25 Apr 2019 (898 words). Resubmitted 31 Jul 2019, report 12 Sep 2019 (544 words). Resubmitted (2) 2 Oct 2019, accepted 7 Oct 2019. Published 12 Oct 2019 (December 2019 edition).
(5) Reflex motion letter. Submitted 22 Dec 2019, report 22 Jan 2020 (562 words). Resubmitted 26 Jan 2020, report 9 Feb 2020 (137 words). Resubmitted (2) 10 Feb 2020, accepted 11 Feb 2020. Published 13 Feb 2020 (May 2020 edition)
(6) EXP. Submitted 29 Apr 2021, report 21 Jun 2021 (1535 words, Eugene Vasiliev). Resubmitted 4 Dec 2021, accepted 10 Dec 2021. Published 8 January 2022 (March 2022 edition).
(7) Leading arm. Submitted 9 Jun 2021, report 21 Jun 2021. Resubmitted 14 Jan 2022, report 10 Feb 2022 (new reviewer). Resubmitted May 16th, accepted May 17th.


Paper 3 was submitted 19 Mar 2019, report 14 May 2019 (4219 words). Withdrawn 31 May!

Co-authors to MNRAS
(1) MSSA. Submitted 15 Sep 2020, report 5 Oct 2020 (791 words). Resubmitted 16 Dec 2020, accepted 18 Dec 2020.
(2) NewHorizon. Submitted 4 Jun 2021, report 21 Sep 2021 (two reviewers). Resubmitted 23 Dec, accepted 16 Feb 2022.
(3) Sgr. Submitted 7 Jun 2021, report 5 Jul 2021 (581 words, Pau Ramos). Resubmitted 30 Jul 2021, accepted 30 Jul 2021.
(4) LMC dark matter. Submitted 30 Nov 2021, report 7 Feb 2022 (583 words). Resubmitted 16 Mar 2022, accepted 22 Mar 2022.

Reviews for MNRAS


Nature Astronomy experience


Phys Rev D experience
(1) Direct Detection. Submitted 9 Sep 2016, report 27 Sep 2016. Resubmitted 2 Dec 2016, accepted 3 Dec 2016. Published 27 Dec 2016 (December 2016 edition).

How to quantify how much effort went into these? Word count, perhaps?

-->
