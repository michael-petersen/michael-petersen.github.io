<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Peer Review</title>
  <link rel="stylesheet" href="../css/main.css">
</head>

<body class="home color-1">
  <div id="headerstatic">
    <div class="container">
      <div class="row">
      	<h2 id="logostatic">
          <a href="../index.html">
      	    <span>Michael S. Petersen</span>
      	    <span class="highlightstatic">Institut d'Astrophysique de Paris</span>
      	  </a>
        </h2>
      </div>
    </div>
  </div>
  <div class="section">
    <div class="container">
      <div class="content">
        <div class="row">
	        <div class="title">
            <h4><a href="list.html">(back to notes list)</a></h4>
          </div>
          <div class="title">
            <h2>Peer Review</h2>
        	    <p>Peer review is an incredibly important part of the scientific process. When functioning well, the community gains high levels of confidence about science products. When functioning badly, peer review has the capacity to grind the science machine completely to a halt. I've personally had mixed experiences on both sides of the process, and I'm starting to try and articulate what makes a good or bad process. I think we're incredibly privileged as scientists to produce work that will exist in perpetuity (particularly in our hyper-connected modern age). Therefore, we should endeavor to create the best products and the best experience. This post is <i>not</i> about the quality of science, per se (although that's inescapable), but rather about the reviewing experience.</p>
              <br>
              <p>I'm going to try and do a couple of things. First, consolidate the timeline data on how the process has gone for different papers. A lot of the delay has been on me, as is readily apparent. If you're reading this and think your process is taking a long time, don't worry, I've taken longer.</p>
              <br>
              <p>I'm also hoping to use this post as a jumping off place for various library science considerations. While finishing up my PhD, I had an opportunity to give a talk relating to library science, and I found a whole world of astronomy that I didn't know about before. For a taste, check out the output of <a href="https://ui.adsabs.harvard.edu/search/q=%20%20author%3A%22%5Ekurtz%2C%20m%22&sort=date%20desc%2C%20bibcode%20desc&p_=0">Michael Kurtz</a>. Other systems, like Altmetric, are gaining popularity, and could possibly be used as ranking metrics in the future.</p>
              <br>
              <p>My long-term (humble) goal is to shift the evaluation of scientists somewhat. Peer review is a direct evaluation of scientists on some level; we also all already know that citations and paper counts (and h-index) have become the primary currency of grading someone's science.</p>
              <br>
              <p>This post was inspired by a <a href="https://twitter.com/rareflwr41/status/1505917366961647623">Twitter thread</a> that I read recently discussing the peer-review process and the role antagonistic reviewers could play in gatekeeping. I'm not totally sure I agree, but the discussion points are thought-provoking.</p>
        	</div>
        </div>
      </div>
    </div>
  </div>

  <div id="footer"> petersen@iap.fr </div>

</body>
</html>
