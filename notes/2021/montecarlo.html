<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Notes</title>
  <link rel="stylesheet" href="../../css/main.css">
</head>

<!--
TODO:
-demonstrate sensitivity to the random number seed (set the random number seed!)
-->

<body class="home color-1">
  <div id="headerstatic">
    <div class="container">
      <div class="row">
      	<h2 id="logostatic">
          <a href="../../index.html">
      	    <span>Michael S. Petersen</span>
      	    <span class="highlightstatic">University of Edinburgh</span>
      	  </a>
        </h2>
      </div>
    </div>
  </div>

  <div class="section" id="research">
    <div class="container">
      <div class="content">
        <div class="row">
	        <div class="title">
            <h4><a href="../list.html">(back to notes list)</a></h4>
          </div>
          <div class="title">
            <h2>Monte Carlo Example</h2>
  	        <p>I've discussed Monte Carlo enough times that it seemed prudent to put together a short pedagogical tool. The full example can be found as a GitHub gist <a href="https://gist.github.com/michael-petersen/d38a2a3b9f8d0f516a370c2a817c1c26l">here</a>, but I'll try to explain a bit better in this note.</p>
            <br>
            <p><b>History.</b> The name Monte Carlo was coined by Metropoulos/Hastings, but the idea goes back further. Henon attributes the concept to von Neumann ('the classical von Neumann rejection technique', Hammersley and Handscomb, 1964).</p>
            <p>Our goal in this exercise is to create a distribution by randomly selecting values that come from some parent distribution. We'll stick to Gaussians in this study, but we could extend to other distributions as well.</p>
            <br>
            <p>Let's try a very simple example: drawing points from a normal distribution and comparing to a Gaussian, to get a feel for convergence with particle draws.</p>
  	        <br>
  	        <pre><code>import numpy as np; import matplotlib.pyplot as plt
# first, set up the gaussian we want to approximate
gaussmean = 0.0
gaussdisp = 0.3
# select sample points
nbins = 20; xgauss = np.linspace(-1.,1.,nbins)
# use the formula for a gaussian pdf
ygauss = (1./(gaussdisp*np.sqrt(2*np.pi)))*np.exp(-(xgauss-gaussmean)**2./(2*gaussdisp**2.))
plt.plot(xgauss,ygauss,color='black') # plot</code></pre>
            <p>So far, so good. But what if we didn't know the pdf, and we wanted to build it up from scratch? This is where Monte Carlo comes in.</p>
            <pre><code># pick the number of samples: the more samples, the more accurate, BUT, the more time. try a few?
nsamples = 100
sample_values = np.random.normal(gaussmean,gaussdisp,nsamples) # select the list of random variates

# put the sample points in bins, count how many in each bin, and normalise (histogram construction)
sbins = np.round(((sample_values - np.nanmin(xgauss))/(xgauss[1]-xgauss[0]))).astype('int')
sample_dist = np.zeros(len(xgauss))
for b in range(0,len(xgauss)): sample_dist[b] += len(np.where(sbins==b)[0])/(nsamples*(xgauss[1]-xgauss[0]))

# plot for comparison to the analytic case
plt.plot(xgauss,sample_dist,color='red',linestyle='dashed')</code></pre>
  	        <p>If you've chosen a large enough number of samples, the Monte Carlo histogram will converge to the true Gaussian pdf. This is obviously a contrived example (we won't always know how to pull from the distribution!), but this is an illustrative step.</p>
        </div>
    	</div>
     </div>
   </div>
 </div>

 <div id="footer"> michael.petersen@roe.ac.uk </div>

</body>
</html>
